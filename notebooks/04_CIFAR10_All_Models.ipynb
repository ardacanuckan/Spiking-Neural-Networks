{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-10 SNN Benchmark: All Architectures\n",
        "\n",
        "Trains and evaluates all five SNN architectures on CIFAR-10 under identical conditions.\n",
        "\n",
        "**Models:**\n",
        "1. Baseline LIF (control)\n",
        "2. DASNN — Dendritic multi-branch neurons with heterogeneous time constants\n",
        "3. Spiking-KAN — Chebyshev polynomial activations\n",
        "4. NEXUS-SNN — Adaptive thresholds + ChebyKAN + skip connections\n",
        "5. APEX-SNN — Parametric LIF + progressive sparsity + multi-scale readout\n",
        "6. ANN Baseline (non-spiking reference)\n",
        "\n",
        "**Protocol:**\n",
        "- Same convolutional backbone (3 blocks, 64→128→256 channels)\n",
        "- Adam optimizer, lr=1e-3, cosine annealing over 50 epochs\n",
        "- 4 time steps for all SNN models\n",
        "- Standard CIFAR-10 augmentation (random crop + horizontal flip)\n",
        "\n",
        "**Metrics:** Test accuracy, spike rate, activation sparsity, parameter count, training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import json\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Core SNN Components\n",
        "\n",
        "Surrogate gradient (ATan) and neuron models shared across all architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Surrogate Gradient ──────────────────────────────────────────────────────\n",
        "\n",
        "class ATanSurrogate(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha=2.0):\n",
        "        ctx.save_for_backward(x)\n",
        "        ctx.alpha = alpha\n",
        "        return (x >= 0).float()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        alpha = ctx.alpha\n",
        "        grad = alpha / (2 * (1 + (np.pi / 2 * alpha * x) ** 2))\n",
        "        return grad * grad_output, None\n",
        "\n",
        "def spike_fn(x):\n",
        "    return ATanSurrogate.apply(x, 2.0)\n",
        "\n",
        "\n",
        "# ── Standard LIF Neuron ────────────────────────────────────────────────────\n",
        "\n",
        "class LIFNeuron(nn.Module):\n",
        "    \"\"\"Leaky integrate-and-fire with soft reset.\"\"\"\n",
        "    def __init__(self, tau=2.0):\n",
        "        super().__init__()\n",
        "        self.beta = 1.0 - 1.0 / tau\n",
        "        self.v = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.v = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.v is None:\n",
        "            self.v = torch.zeros_like(x)\n",
        "        self.v = self.beta * self.v + (1 - self.beta) * x\n",
        "        spike = spike_fn(self.v - 1.0)\n",
        "        self.v = self.v - spike.detach()\n",
        "        return spike\n",
        "\n",
        "\n",
        "# ── Adaptive LIF (learnable threshold per channel) ─────────────────────────\n",
        "\n",
        "class AdaptiveLIFNeuron(nn.Module):\n",
        "    def __init__(self, channels, tau=2.0):\n",
        "        super().__init__()\n",
        "        self.beta = 1.0 - 1.0 / tau\n",
        "        self.threshold = nn.Parameter(torch.ones(channels))\n",
        "        self.v = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.v = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.v is None:\n",
        "            self.v = torch.zeros_like(x)\n",
        "        self.v = self.beta * self.v + (1 - self.beta) * x\n",
        "        thr = self.threshold.abs()\n",
        "        if x.dim() == 4:\n",
        "            thr = thr.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            thr = thr.view(1, -1)\n",
        "        spike = spike_fn(self.v - thr)\n",
        "        self.v = self.v - spike.detach() * thr\n",
        "        return spike\n",
        "\n",
        "\n",
        "# ── Parametric LIF (learnable tau per channel) ─────────────────────────────\n",
        "\n",
        "class ParametricLIFNeuron(nn.Module):\n",
        "    def __init__(self, channels, tau_init=2.0):\n",
        "        super().__init__()\n",
        "        self.log_tau = nn.Parameter(torch.full((channels,), np.log(tau_init)))\n",
        "        self.v = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.v = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        tau = torch.exp(self.log_tau).clamp(1.1, 20.0)\n",
        "        beta = 1.0 - 1.0 / tau\n",
        "        if x.dim() == 4:\n",
        "            beta = beta.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            beta = beta.view(1, -1)\n",
        "        if self.v is None:\n",
        "            self.v = torch.zeros_like(x)\n",
        "        self.v = beta * self.v + (1 - beta) * x\n",
        "        spike = spike_fn(self.v - 1.0)\n",
        "        self.v = self.v - spike.detach()\n",
        "        return spike\n",
        "\n",
        "print('Neuron models ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Shared Modules\n",
        "\n",
        "Chebyshev KAN activation and progressive sparsity layer, used by NEXUS and APEX."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChebyshevKAN(nn.Module):\n",
        "    \"\"\"Channel-wise Chebyshev polynomial transformation.\n",
        "    Replaces fixed activations with learnable polynomials.\"\"\"\n",
        "\n",
        "    def __init__(self, channels, degree=3):\n",
        "        super().__init__()\n",
        "        self.degree = degree\n",
        "        self.coefficients = nn.Parameter(torch.zeros(channels, degree + 1))\n",
        "        nn.init.constant_(self.coefficients[:, 1], 1.0)\n",
        "        nn.init.normal_(self.coefficients[:, 2:], 0, 0.01)\n",
        "        self.residual_weight = nn.Parameter(torch.tensor(0.3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        c = self.coefficients\n",
        "        x_norm = torch.tanh(x * 0.3)\n",
        "        T_prev = torch.ones_like(x_norm)\n",
        "        T_curr = x_norm\n",
        "\n",
        "        if x.dim() == 4:\n",
        "            reshape = lambda v: v.view(1, -1, 1, 1)\n",
        "        else:\n",
        "            reshape = lambda v: v\n",
        "\n",
        "        result = reshape(c[:, 0]) * T_prev\n",
        "        if self.degree >= 1:\n",
        "            result = result + reshape(c[:, 1]) * T_curr\n",
        "        for i in range(2, self.degree + 1):\n",
        "            T_next = 2 * x_norm * T_curr - T_prev\n",
        "            result = result + reshape(c[:, i]) * T_next\n",
        "            T_prev, T_curr = T_curr, T_next\n",
        "\n",
        "        return result + self.residual_weight * x\n",
        "\n",
        "\n",
        "class SparsityLayer(nn.Module):\n",
        "    \"\"\"Progressive sparsity: prunes a growing fraction of channels during training.\"\"\"\n",
        "\n",
        "    def __init__(self, channels, target_sparsity=0.7):\n",
        "        super().__init__()\n",
        "        self.importance = nn.Parameter(torch.ones(channels))\n",
        "        self.target_sparsity = target_sparsity\n",
        "        self.progress = 0.0\n",
        "\n",
        "    def set_progress(self, progress):\n",
        "        self.progress = progress\n",
        "\n",
        "    def forward(self, x):\n",
        "        importance = torch.sigmoid(self.importance)\n",
        "        sparsity = 0.2 + self.progress * (self.target_sparsity - 0.2)\n",
        "        if self.training:\n",
        "            mask = importance\n",
        "        else:\n",
        "            threshold = torch.quantile(importance, sparsity)\n",
        "            mask = (importance >= threshold).float()\n",
        "        if x.dim() == 4:\n",
        "            return x * mask.view(1, -1, 1, 1)\n",
        "        return x * mask.view(1, -1)\n",
        "\n",
        "print('Shared modules ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Model Definitions\n",
        "\n",
        "All models share the same convolutional backbone shape:\n",
        "- 3 conv blocks (64 → 128 → 256 channels), each with Conv2d(3×3) + BN + Spike + AvgPool(2)\n",
        "- FC head: 4096 → 512 → 10\n",
        "\n",
        "The difference is **which neuron type** and **which extra modules** each model uses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Baseline LIF (Control)\n",
        "Standard LIF neurons. No bells and whistles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaselineSNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, tau=2.0):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.lif1  = LIFNeuron(tau)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.lif2  = LIFNeuron(tau)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.lif3  = LIFNeuron(tau)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.lif4 = LIFNeuron(tau)\n",
        "        self.fc2  = nn.Linear(512, num_classes)\n",
        "\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = []\n",
        "        h = self.lif1(self.bn1(self.conv1(x))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif2(self.bn2(self.conv2(h))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif3(self.bn3(self.conv3(h))); spikes.append(h); h = self.pool(h)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.lif4(self.fc1(h)); spikes.append(h)\n",
        "        return self.fc2(h), spikes\n",
        "\n",
        "print(f'BaselineSNN params: {sum(p.numel() for p in BaselineSNN().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 DASNN — Dendritic Attention SNN\n",
        "\n",
        "Each spiking neuron is replaced by a **multi-branch dendritic neuron**:\n",
        "- 4 branches with different membrane time constants (τ ∈ [1.5, 8.0])\n",
        "- Input-dependent gating selects which branch contributes at each spatial position\n",
        "- The soma integrates the gated branches and fires if threshold is exceeded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DendriticConvLIF(nn.Module):\n",
        "    \"\"\"Multi-branch conv LIF with heterogeneous time constants and gating.\"\"\"\n",
        "    def __init__(self, channels, num_branches=4, tau_range=(1.5, 8.0)):\n",
        "        super().__init__()\n",
        "        self.num_branches = num_branches\n",
        "        taus = np.linspace(tau_range[0], tau_range[1], num_branches)\n",
        "        self.register_buffer('betas', torch.tensor([1.0 - 1.0 / t for t in taus]))\n",
        "        self.gate = nn.Conv2d(channels, num_branches, 1)\n",
        "        self.branch_v = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.branch_v = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.branch_v is None:\n",
        "            self.branch_v = [torch.zeros_like(x) for _ in range(self.num_branches)]\n",
        "        for i in range(self.num_branches):\n",
        "            b = self.betas[i]\n",
        "            self.branch_v[i] = b * self.branch_v[i] + (1 - b) * x\n",
        "        gate = torch.softmax(self.gate(x), dim=1)          # [B, branches, H, W]\n",
        "        soma = sum(gate[:, i:i+1] * self.branch_v[i]       # [B, 1, H, W] * [B, C, H, W]\n",
        "                   for i in range(self.num_branches))\n",
        "        spike = spike_fn(soma - 1.0)\n",
        "        for i in range(self.num_branches):\n",
        "            self.branch_v[i] = self.branch_v[i] - spike.detach()\n",
        "        return spike\n",
        "\n",
        "\n",
        "class DASNN_CIFAR(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, num_branches=4):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.lif1  = DendriticConvLIF(64, num_branches)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.lif2  = DendriticConvLIF(128, num_branches)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.lif3  = DendriticConvLIF(256, num_branches)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.lif4 = LIFNeuron(2.0)\n",
        "        self.fc2  = nn.Linear(512, num_classes)\n",
        "\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = []\n",
        "        h = self.lif1(self.bn1(self.conv1(x))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif2(self.bn2(self.conv2(h))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif3(self.bn3(self.conv3(h))); spikes.append(h); h = self.pool(h)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.lif4(self.fc1(h)); spikes.append(h)\n",
        "        return self.fc2(h), spikes\n",
        "\n",
        "print(f'DASNN params: {sum(p.numel() for p in DASNN_CIFAR().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Spiking-KAN\n",
        "\n",
        "Inserts a **Chebyshev KAN activation** between batch norm and the LIF neuron in every layer.\n",
        "The polynomial transformation gives each channel a learnable input-output curve before spiking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpikingKAN_CIFAR(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, tau=2.0, degree=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.kan1  = ChebyshevKAN(64, degree)\n",
        "        self.lif1  = LIFNeuron(tau)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.kan2  = ChebyshevKAN(128, degree)\n",
        "        self.lif2  = LIFNeuron(tau)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.kan3  = ChebyshevKAN(256, degree)\n",
        "        self.lif3  = LIFNeuron(tau)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.lif4 = LIFNeuron(tau)\n",
        "        self.fc2  = nn.Linear(512, num_classes)\n",
        "\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = []\n",
        "        h = self.lif1(self.kan1(self.bn1(self.conv1(x)))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif2(self.kan2(self.bn2(self.conv2(h)))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif3(self.kan3(self.bn3(self.conv3(h)))); spikes.append(h); h = self.pool(h)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.lif4(self.fc1(h)); spikes.append(h)\n",
        "        return self.fc2(h), spikes\n",
        "\n",
        "print(f'SpikingKAN params: {sum(p.numel() for p in SpikingKAN_CIFAR().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 NEXUS-SNN\n",
        "\n",
        "Combines three ideas:\n",
        "- **Adaptive LIF** neurons with per-channel learnable thresholds\n",
        "- **ChebyKAN** activations for richer representations\n",
        "- **Skip connection** from the first layer to the FC head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NexusSNN_CIFAR(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, tau=2.0, degree=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.kan1  = ChebyshevKAN(64, degree)\n",
        "        self.lif1  = AdaptiveLIFNeuron(64, tau)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.kan2  = ChebyshevKAN(128, degree)\n",
        "        self.lif2  = AdaptiveLIFNeuron(128, tau)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.kan3  = ChebyshevKAN(256, degree)\n",
        "        self.lif3  = AdaptiveLIFNeuron(256, tau)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        # Skip: global pool first-layer features → project to FC width\n",
        "        self.skip = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(64, 512)\n",
        "        )\n",
        "        self.skip_weight = nn.Parameter(torch.tensor(0.2))\n",
        "\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.lif4 = AdaptiveLIFNeuron(512, tau)\n",
        "        self.fc2  = nn.Linear(512, num_classes)\n",
        "\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = []\n",
        "        h = self.lif1(self.kan1(self.bn1(self.conv1(x)))); spikes.append(h)\n",
        "        skip_h = h\n",
        "        h = self.pool(h)\n",
        "        h = self.lif2(self.kan2(self.bn2(self.conv2(h)))); spikes.append(h); h = self.pool(h)\n",
        "        h = self.lif3(self.kan3(self.bn3(self.conv3(h)))); spikes.append(h); h = self.pool(h)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.fc1(h) + self.skip_weight * self.skip(skip_h)\n",
        "        h = self.lif4(h); spikes.append(h)\n",
        "        return self.fc2(h), spikes\n",
        "\n",
        "print(f'NEXUS-SNN params: {sum(p.numel() for p in NexusSNN_CIFAR().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 APEX-SNN\n",
        "\n",
        "Stacks everything:\n",
        "- **Parametric LIF** — per-channel learnable time constant τ\n",
        "- **ChebyKAN** activations\n",
        "- **Progressive sparsity** — increasingly masks out channels during training\n",
        "- **Multi-scale readout** — ensemble of classifiers attached at every layer depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ApexSNN_CIFAR(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10, tau=2.0, degree=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, 3, padding=1)\n",
        "        self.bn1   = nn.BatchNorm2d(64)\n",
        "        self.kan1  = ChebyshevKAN(64, degree)\n",
        "        self.sp1   = SparsityLayer(64)\n",
        "        self.lif1  = ParametricLIFNeuron(64, tau)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn2   = nn.BatchNorm2d(128)\n",
        "        self.kan2  = ChebyshevKAN(128, degree)\n",
        "        self.sp2   = SparsityLayer(128)\n",
        "        self.lif2  = ParametricLIFNeuron(128, tau)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.bn3   = nn.BatchNorm2d(256)\n",
        "        self.kan3  = ChebyshevKAN(256, degree)\n",
        "        self.sp3   = SparsityLayer(256)\n",
        "        self.lif3  = ParametricLIFNeuron(256, tau)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "        self.fc1  = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.lif4 = ParametricLIFNeuron(512, tau)\n",
        "\n",
        "        # Multi-scale readouts\n",
        "        self.readout1  = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(64, num_classes))\n",
        "        self.readout2  = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(128, num_classes))\n",
        "        self.readout3  = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(256, num_classes))\n",
        "        self.readout_fc = nn.Linear(512, num_classes)\n",
        "        self.ensemble_w = nn.Parameter(torch.ones(4) / 4)\n",
        "\n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "\n",
        "    def set_epoch(self, epoch, total_epochs):\n",
        "        progress = epoch / (total_epochs + 1)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, SparsityLayer):\n",
        "                m.set_progress(progress)\n",
        "\n",
        "    def forward(self, x):\n",
        "        spikes = []\n",
        "        h = self.lif1(self.sp1(self.kan1(self.bn1(self.conv1(x))))); spikes.append(h)\n",
        "        h1 = h; h = self.pool(h)\n",
        "        h = self.lif2(self.sp2(self.kan2(self.bn2(self.conv2(h))))); spikes.append(h)\n",
        "        h2 = h; h = self.pool(h)\n",
        "        h = self.lif3(self.sp3(self.kan3(self.bn3(self.conv3(h))))); spikes.append(h)\n",
        "        h3 = h; h = self.pool(h)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = self.lif4(self.fc1(h)); spikes.append(h)\n",
        "\n",
        "        w = F.softmax(self.ensemble_w, dim=0)\n",
        "        out = (w[0] * self.readout1(h1) + w[1] * self.readout2(h2) +\n",
        "               w[2] * self.readout3(h3) + w[3] * self.readout_fc(h))\n",
        "        return out, spikes\n",
        "\n",
        "print(f'APEX-SNN params: {sum(p.numel() for p in ApexSNN_CIFAR().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 ANN Baseline (non-spiking)\n",
        "\n",
        "Same convolutional backbone with ReLU instead of spiking neurons. Single forward pass (no time steps)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ANN_Baseline(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.AvgPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.AvgPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.AvgPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 4 * 4, 512), nn.ReLU(), nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.features(x)\n",
        "        return self.classifier(h.view(h.size(0), -1))\n",
        "\n",
        "print(f'ANN params: {sum(p.numel() for p in ANN_Baseline().parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Data Loading\n",
        "\n",
        "CIFAR-10 with standard augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "train_data = datasets.CIFAR10('./data', train=True,  download=True, transform=train_transform)\n",
        "test_data  = datasets.CIFAR10('./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=128, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "print(f'Train: {len(train_data):,}  |  Test: {len(test_data):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── SNN training (multi-timestep) ──────────────────────────────────────────\n",
        "\n",
        "def train_snn_epoch(model, loader, optimizer, device, timesteps=4):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        model.reset()\n",
        "        outputs = []\n",
        "        for t in range(timesteps):\n",
        "            out, _ = model(data)\n",
        "            outputs.append(out)\n",
        "        logits = torch.stack(outputs).mean(0)\n",
        "        loss = F.cross_entropy(logits, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * target.size(0)\n",
        "        correct += logits.argmax(1).eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    return total_loss / total, 100.0 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_snn(model, loader, device, timesteps=4):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    total_spikes, total_elements = 0.0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        model.reset()\n",
        "        outputs = []\n",
        "        for t in range(timesteps):\n",
        "            out, spikes = model(data)\n",
        "            outputs.append(out)\n",
        "            for s in spikes:\n",
        "                total_spikes += s.sum().item()\n",
        "                total_elements += s.numel()\n",
        "        logits = torch.stack(outputs).mean(0)\n",
        "        correct += logits.argmax(1).eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    spike_rate = total_spikes / total_elements if total_elements > 0 else 0\n",
        "    return accuracy, spike_rate\n",
        "\n",
        "\n",
        "# ── ANN training (single pass) ────────────────────────────────────────────\n",
        "\n",
        "def train_ann_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(data)\n",
        "        loss = F.cross_entropy(logits, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * target.size(0)\n",
        "        correct += logits.argmax(1).eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    return total_loss / total, 100.0 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_ann(model, loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        logits = model(data)\n",
        "        correct += logits.argmax(1).eq(target).sum().item()\n",
        "        total += target.size(0)\n",
        "    return 100.0 * correct / total\n",
        "\n",
        "print('Training functions ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Run Benchmark\n",
        "\n",
        "Train all 6 models sequentially. Each gets 50 epochs with Adam + cosine annealing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS    = 50\n",
        "TIMESTEPS = 4\n",
        "LR        = 1e-3\n",
        "\n",
        "snn_configs = [\n",
        "    ('Baseline LIF', BaselineSNN()),\n",
        "    ('DASNN',        DASNN_CIFAR()),\n",
        "    ('Spiking-KAN',  SpikingKAN_CIFAR()),\n",
        "    ('NEXUS-SNN',    NexusSNN_CIFAR()),\n",
        "    ('APEX-SNN',     ApexSNN_CIFAR()),\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in snn_configs:\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'  {name}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    model = model.to(device)\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Parameters: {params:,}')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)\n",
        "\n",
        "    best_acc, best_spike = 0.0, 0.0\n",
        "    history = {'train': [], 'test': [], 'spike': []}\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        if hasattr(model, 'set_epoch'):\n",
        "            model.set_epoch(epoch, EPOCHS)\n",
        "\n",
        "        _, train_acc = train_snn_epoch(model, train_loader, optimizer, device, TIMESTEPS)\n",
        "        test_acc, spike_rate = eval_snn(model, test_loader, device, TIMESTEPS)\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train'].append(train_acc)\n",
        "        history['test'].append(test_acc)\n",
        "        history['spike'].append(spike_rate)\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_spike = spike_rate\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "            print(f'  Epoch {epoch+1:3d}/{EPOCHS} | '\n",
        "                  f'Train {train_acc:.2f}% | Test {test_acc:.2f}% | '\n",
        "                  f'Spike {spike_rate:.4f} | Best {best_acc:.2f}%')\n",
        "\n",
        "    elapsed = time.time() - t0\n",
        "    results[name] = dict(\n",
        "        accuracy=best_acc, spike_rate=best_spike, sparsity=1-best_spike,\n",
        "        parameters=params, timesteps=TIMESTEPS, time=elapsed, history=history,\n",
        "    )\n",
        "    print(f'\\n  >> {name}: {best_acc:.2f}%  ({elapsed:.0f}s)')\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── ANN Baseline ──────────────────────────────────────────────────────────\n",
        "print(f'\\n{\"=\"*60}')\n",
        "print(f'  ANN Baseline')\n",
        "print(f'{\"=\"*60}')\n",
        "\n",
        "ann = ANN_Baseline().to(device)\n",
        "ann_params = sum(p.numel() for p in ann.parameters())\n",
        "print(f'Parameters: {ann_params:,}')\n",
        "\n",
        "optimizer = optim.Adam(ann.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, EPOCHS)\n",
        "\n",
        "best_ann_acc = 0.0\n",
        "ann_history = {'train': [], 'test': []}\n",
        "t0 = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    _, train_acc = train_ann_epoch(ann, train_loader, optimizer, device)\n",
        "    test_acc = eval_ann(ann, test_loader, device)\n",
        "    scheduler.step()\n",
        "    ann_history['train'].append(train_acc)\n",
        "    ann_history['test'].append(test_acc)\n",
        "    if test_acc > best_ann_acc:\n",
        "        best_ann_acc = test_acc\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f'  Epoch {epoch+1:3d}/{EPOCHS} | '\n",
        "              f'Train {train_acc:.2f}% | Test {test_acc:.2f}% | Best {best_ann_acc:.2f}%')\n",
        "\n",
        "ann_elapsed = time.time() - t0\n",
        "results['ANN Baseline'] = dict(\n",
        "    accuracy=best_ann_acc, spike_rate=0, sparsity=0,\n",
        "    parameters=ann_params, timesteps=1, time=ann_elapsed, history=ann_history,\n",
        ")\n",
        "print(f'\\n  >> ANN Baseline: {best_ann_acc:.2f}%  ({ann_elapsed:.0f}s)')\n",
        "del ann, optimizer, scheduler\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '=' * 80)\n",
        "print('CIFAR-10 BENCHMARK RESULTS')\n",
        "print('=' * 80)\n",
        "print(f'{\"Model\":<16} {\"Accuracy\":<11} {\"Spike Rate\":<12} {\"Sparsity\":<11} {\"Params\":<11} {\"T\":<4} {\"Time\":<8}')\n",
        "print('-' * 80)\n",
        "\n",
        "for name in ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN', 'ANN Baseline']:\n",
        "    r = results[name]\n",
        "    sr = f'{r[\"spike_rate\"]:.4f}' if r['spike_rate'] > 0 else 'N/A'\n",
        "    sp = f'{r[\"sparsity\"]:.2f}'   if r['sparsity'] > 0 else 'N/A'\n",
        "    print(f'{name:<16} {r[\"accuracy\"]:.2f}%     {sr:<12} {sp:<11} {r[\"parameters\"]:>9,}  {r[\"timesteps\"]:<4} {r[\"time\"]:.0f}s')\n",
        "\n",
        "print('=' * 80)\n",
        "\n",
        "# Gap analysis\n",
        "ann_acc = results['ANN Baseline']['accuracy']\n",
        "print(f'\\nANN Baseline accuracy: {ann_acc:.2f}%')\n",
        "for name in ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN']:\n",
        "    gap = ann_acc - results[name]['accuracy']\n",
        "    print(f'  {name:<16} gap: {gap:+.2f}%')\n",
        "\n",
        "# Energy estimate\n",
        "print(f'\\nTheoretical energy (45nm CMOS, E_AC=0.9pJ, E_MAC=4.6pJ):')\n",
        "for name in ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN']:\n",
        "    sr = results[name]['spike_rate']\n",
        "    ratio = sr * 0.9 / (0.5 * 4.6)  # vs 50% ReLU activation\n",
        "    print(f'  {name:<16} energy ratio: {ratio:.3f}x  (saving: {(1-ratio)*100:.1f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Plots ─────────────────────────────────────────────────────────────────\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "colors = {'Baseline LIF': '#1f77b4', 'DASNN': '#ff7f0e', 'Spiking-KAN': '#2ca02c',\n",
        "          'NEXUS-SNN': '#d62728', 'APEX-SNN': '#9467bd', 'ANN Baseline': '#8c564b'}\n",
        "\n",
        "# 1. Test accuracy curves\n",
        "for name, r in results.items():\n",
        "    axes[0].plot(r['history']['test'], label=name, color=colors[name], linewidth=1.5)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Test Accuracy (%)')\n",
        "axes[0].set_title('Learning Curves')\n",
        "axes[0].legend(fontsize=8)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Accuracy bar chart\n",
        "snn_names = ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN', 'ANN Baseline']\n",
        "accs = [results[n]['accuracy'] for n in snn_names]\n",
        "bar_colors = [colors[n] for n in snn_names]\n",
        "bars = axes[1].bar(range(len(snn_names)), accs, color=bar_colors)\n",
        "axes[1].set_xticks(range(len(snn_names)))\n",
        "axes[1].set_xticklabels(snn_names, rotation=45, ha='right', fontsize=8)\n",
        "axes[1].set_ylabel('Best Test Accuracy (%)')\n",
        "axes[1].set_title('Final Accuracy Comparison')\n",
        "for bar, acc in zip(bars, accs):\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.3,\n",
        "                 f'{acc:.1f}', ha='center', fontsize=8)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Spike rate over training (SNN only)\n",
        "for name in ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN']:\n",
        "    axes[2].plot(results[name]['history']['spike'], label=name, color=colors[name], linewidth=1.5)\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Spike Rate')\n",
        "axes[2].set_title('Spike Rate During Training')\n",
        "axes[2].legend(fontsize=8)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cifar10_all_models.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Accuracy vs Spike Rate scatter ────────────────────────────────────────\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "for name in ['Baseline LIF', 'DASNN', 'Spiking-KAN', 'NEXUS-SNN', 'APEX-SNN']:\n",
        "    r = results[name]\n",
        "    ax.scatter(r['spike_rate'], r['accuracy'], s=r['parameters']/5000,\n",
        "               color=colors[name], label=name, alpha=0.8, edgecolors='black', linewidth=0.5)\n",
        "\n",
        "ax.set_xlabel('Spike Rate (lower = more efficient)')\n",
        "ax.set_ylabel('Test Accuracy (%)')\n",
        "ax.set_title('Accuracy vs Spike Rate (bubble size = parameter count)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cifar10_accuracy_vs_spike.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Save results ──────────────────────────────────────────────────────────\n",
        "\n",
        "results_save = {\n",
        "    name: {k: v for k, v in r.items() if k != 'history'}\n",
        "    for name, r in results.items()\n",
        "}\n",
        "with open('cifar10_benchmark_results.json', 'w') as f:\n",
        "    json.dump(results_save, f, indent=2)\n",
        "\n",
        "print('Results saved to cifar10_benchmark_results.json')\n",
        "\n",
        "# Optional: save to Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import shutil\n",
        "    shutil.copy('cifar10_benchmark_results.json', '/content/drive/MyDrive/')\n",
        "    shutil.copy('cifar10_all_models.png', '/content/drive/MyDrive/')\n",
        "    shutil.copy('cifar10_accuracy_vs_spike.png', '/content/drive/MyDrive/')\n",
        "    print('Saved to Google Drive.')\n",
        "except Exception:\n",
        "    print('Google Drive not available, results saved locally.')"
      ]
    }
  ]
}
