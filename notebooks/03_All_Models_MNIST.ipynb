{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# All SNN Models Comparison on MNIST\n",
        "\n",
        "Models:\n",
        "1. **Baseline LIF** - Standard Leaky Integrate-and-Fire\n",
        "2. **DASNN** - Dendritic Attention SNN\n",
        "3. **Spiking-KAN** - Kolmogorov-Arnold Network with spikes\n",
        "4. **NEXUS-SNN** - Multi-innovation SNN\n",
        "5. **APEX-SNN** - Ultimate combined model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ATanSurrogate(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha=2.0):\n",
        "        ctx.save_for_backward(x)\n",
        "        ctx.alpha = alpha\n",
        "        return (x >= 0).float()\n",
        "    \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        grad = ctx.alpha / (2 * (1 + (np.pi/2 * ctx.alpha * x)**2))\n",
        "        return grad * grad_output, None\n",
        "\n",
        "def spike_fn(x):\n",
        "    return ATanSurrogate.apply(x, 2.0)\n",
        "\n",
        "class LIFNeuron(nn.Module):\n",
        "    def __init__(self, tau=2.0, threshold=1.0):\n",
        "        super().__init__()\n",
        "        self.tau = tau\n",
        "        self.threshold = threshold\n",
        "        self.beta = 1.0 - 1.0 / tau\n",
        "        self.v = None\n",
        "    \n",
        "    def reset(self):\n",
        "        self.v = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.v is None:\n",
        "            self.v = torch.zeros_like(x)\n",
        "        self.v = self.beta * self.v + x\n",
        "        spike = spike_fn(self.v - self.threshold)\n",
        "        self.v = self.v - spike * self.threshold\n",
        "        return spike\n",
        "\n",
        "class AdaptiveLIF(nn.Module):\n",
        "    def __init__(self, size, tau=2.0):\n",
        "        super().__init__()\n",
        "        self.tau = nn.Parameter(torch.ones(size) * tau)\n",
        "        self.threshold = nn.Parameter(torch.ones(size))\n",
        "        self.v = None\n",
        "    \n",
        "    def reset(self):\n",
        "        self.v = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.v is None:\n",
        "            self.v = torch.zeros_like(x)\n",
        "        beta = 1.0 - 1.0 / self.tau.abs().clamp(min=1.1)\n",
        "        self.v = beta * self.v + x\n",
        "        spike = spike_fn(self.v - self.threshold.abs())\n",
        "        self.v = self.v - spike * self.threshold.abs()\n",
        "        return spike"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 1: Baseline LIF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaselineLIF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.lif1 = LIFNeuron()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.lif2 = LIFNeuron()\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.lif1.reset()\n",
        "        self.lif2.reset()\n",
        "    \n",
        "    def forward(self, x, timesteps=4):\n",
        "        self.reset()\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = []\n",
        "        spikes = []\n",
        "        \n",
        "        for t in range(timesteps):\n",
        "            h = self.lif1(self.fc1(x))\n",
        "            spikes.append(h.detach())\n",
        "            h = self.lif2(self.fc2(h))\n",
        "            spikes.append(h.detach())\n",
        "            out = self.fc3(h)\n",
        "            outputs.append(out)\n",
        "        \n",
        "        spike_rate = torch.cat([s.flatten() for s in spikes]).mean().item()\n",
        "        return torch.stack(outputs).mean(0), spike_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 2: DASNN (Dendritic Attention SNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DendriticNeuron(nn.Module):\n",
        "    def __init__(self, in_features, out_features, n_branches=4):\n",
        "        super().__init__()\n",
        "        self.n_branches = n_branches\n",
        "        self.branches = nn.ModuleList([\n",
        "            nn.Linear(in_features, out_features) for _ in range(n_branches)\n",
        "        ])\n",
        "        self.gate = nn.Linear(in_features, n_branches)\n",
        "        self.lif = LIFNeuron()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.lif.reset()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        gates = torch.softmax(self.gate(x), dim=-1)\n",
        "        branch_outs = torch.stack([b(x) for b in self.branches], dim=-1)\n",
        "        weighted = (branch_outs * gates.unsqueeze(1)).sum(-1)\n",
        "        return self.lif(weighted)\n",
        "\n",
        "class DASNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dn1 = DendriticNeuron(784, 512)\n",
        "        self.dn2 = DendriticNeuron(512, 256)\n",
        "        self.fc = nn.Linear(256, 10)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.dn1.reset()\n",
        "        self.dn2.reset()\n",
        "    \n",
        "    def forward(self, x, timesteps=4):\n",
        "        self.reset()\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = []\n",
        "        spikes = []\n",
        "        \n",
        "        for t in range(timesteps):\n",
        "            h = self.dn1(x)\n",
        "            spikes.append(h.detach())\n",
        "            h = self.dn2(h)\n",
        "            spikes.append(h.detach())\n",
        "            outputs.append(self.fc(h))\n",
        "        \n",
        "        spike_rate = torch.cat([s.flatten() for s in spikes]).mean().item()\n",
        "        return torch.stack(outputs).mean(0), spike_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 3: Spiking-KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpikingKANLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, degree=3):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.degree = degree\n",
        "        \n",
        "        self.coeffs = nn.Parameter(torch.randn(in_features, out_features, degree + 1) * 0.1)\n",
        "        self.lif = LIFNeuron()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.lif.reset()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x_expanded = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        \n",
        "        powers = torch.stack([x_expanded.pow(i) for i in range(self.degree + 1)], dim=-1)\n",
        "        powers = powers.squeeze(-2)\n",
        "        \n",
        "        out = torch.einsum('bi,iod,bid->bo', x, self.coeffs, powers)\n",
        "        return self.lif(out)\n",
        "\n",
        "class SpikingKAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.kan1 = SpikingKANLayer(784, 256, degree=3)\n",
        "        self.kan2 = SpikingKANLayer(256, 128, degree=3)\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.kan1.reset()\n",
        "        self.kan2.reset()\n",
        "    \n",
        "    def forward(self, x, timesteps=4):\n",
        "        self.reset()\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = []\n",
        "        spikes = []\n",
        "        \n",
        "        for t in range(timesteps):\n",
        "            h = self.kan1(x)\n",
        "            spikes.append(h.detach())\n",
        "            h = self.kan2(h)\n",
        "            spikes.append(h.detach())\n",
        "            outputs.append(self.fc(h))\n",
        "        \n",
        "        spike_rate = torch.cat([s.flatten() for s in spikes]).mean().item()\n",
        "        return torch.stack(outputs).mean(0), spike_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 4: NEXUS-SNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TemporalAttention(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.scale = dim ** -0.5\n",
        "    \n",
        "    def forward(self, x_list):\n",
        "        if len(x_list) == 1:\n",
        "            return x_list[0]\n",
        "        x = torch.stack(x_list, dim=1)\n",
        "        q, k, v = self.query(x), self.key(x), self.value(x)\n",
        "        attn = torch.softmax(torch.bmm(q, k.transpose(-1, -2)) * self.scale, dim=-1)\n",
        "        return torch.bmm(attn, v)[:, -1]\n",
        "\n",
        "class NEXUSSNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.lif1 = AdaptiveLIF(512)\n",
        "        \n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.lif2 = AdaptiveLIF(256)\n",
        "        \n",
        "        self.attn = TemporalAttention(256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "    \n",
        "    def reset(self):\n",
        "        self.lif1.reset()\n",
        "        self.lif2.reset()\n",
        "    \n",
        "    def forward(self, x, timesteps=6):\n",
        "        self.reset()\n",
        "        x = x.view(x.size(0), -1)\n",
        "        outputs = []\n",
        "        hidden_states = []\n",
        "        spikes = []\n",
        "        \n",
        "        for t in range(timesteps):\n",
        "            h = self.lif1(self.bn1(self.fc1(x)))\n",
        "            spikes.append(h.detach())\n",
        "            h = self.lif2(self.bn2(self.fc2(h)))\n",
        "            spikes.append(h.detach())\n",
        "            hidden_states.append(h)\n",
        "            \n",
        "            attended = self.attn(hidden_states)\n",
        "            outputs.append(self.fc3(attended))\n",
        "        \n",
        "        spike_rate = torch.cat([s.flatten() for s in spikes]).mean().item()\n",
        "        return torch.stack(outputs).mean(0), spike_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model 5: APEX-SNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class APEXSNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.lif1 = LIFNeuron()\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.lif2 = LIFNeuron()\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.lif3 = LIFNeuron()\n",
        "        \n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.lif4 = AdaptiveLIF(256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "        \n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "    \n",
        "    def reset(self):\n",
        "        for m in self.modules():\n",
        "            if hasattr(m, 'reset') and m is not self:\n",
        "                m.reset()\n",
        "    \n",
        "    def forward(self, x, timesteps=4):\n",
        "        self.reset()\n",
        "        if x.dim() == 3:\n",
        "            x = x.unsqueeze(1)\n",
        "        \n",
        "        outputs = []\n",
        "        spikes = []\n",
        "        \n",
        "        for t in range(timesteps):\n",
        "            h = self.lif1(self.bn1(self.conv1(x)))\n",
        "            spikes.append(h.detach())\n",
        "            h = self.pool(h)\n",
        "            \n",
        "            h = self.lif2(self.bn2(self.conv2(h)))\n",
        "            spikes.append(h.detach())\n",
        "            h = self.pool(h)\n",
        "            \n",
        "            h = self.lif3(self.bn3(self.conv3(h)))\n",
        "            spikes.append(h.detach())\n",
        "            h = self.pool(h)\n",
        "            \n",
        "            h = h.view(h.size(0), -1)\n",
        "            h = self.lif4(self.fc1(h))\n",
        "            spikes.append(h.detach())\n",
        "            outputs.append(self.fc2(h))\n",
        "        \n",
        "        spike_rate = torch.cat([s.flatten() for s in spikes]).mean().item()\n",
        "        return torch.stack(outputs).mean(0), spike_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f'Train: {len(train_data)}, Test: {len(test_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, test_loader, epochs=20, lr=1e-3, timesteps=4):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "    \n",
        "    history = {'acc': [], 'spike_rate': []}\n",
        "    best_acc = 0\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(data, timesteps)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "        \n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        spike_rates = []\n",
        "        with torch.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output, spike_rate = model(data, timesteps)\n",
        "                spike_rates.append(spike_rate)\n",
        "                pred = output.argmax(dim=1)\n",
        "                correct += pred.eq(target).sum().item()\n",
        "                total += target.size(0)\n",
        "        \n",
        "        acc = 100. * correct / total\n",
        "        avg_spike = np.mean(spike_rates)\n",
        "        history['acc'].append(acc)\n",
        "        history['spike_rate'].append(avg_spike)\n",
        "        \n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "        \n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f'Epoch {epoch+1:2d} | Acc: {acc:.2f}% | Spike: {avg_spike:.4f}')\n",
        "    \n",
        "    return best_acc, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EPOCHS = 20\n",
        "results = {}\n",
        "\n",
        "models = {\n",
        "    'Baseline LIF': (BaselineLIF(), 4),\n",
        "    'DASNN': (DASNN(), 4),\n",
        "    'Spiking-KAN': (SpikingKAN(), 4),\n",
        "    'NEXUS-SNN': (NEXUSSNN(), 6),\n",
        "    'APEX-SNN': (APEXSNN(), 4),\n",
        "}\n",
        "\n",
        "for name, (model, timesteps) in models.items():\n",
        "    print(f'\\n{\"=\"*50}')\n",
        "    print(f'Training {name}')\n",
        "    print(f'{\"=\"*50}')\n",
        "    \n",
        "    model = model.to(device)\n",
        "    params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Parameters: {params:,}')\n",
        "    \n",
        "    t0 = time.time()\n",
        "    best_acc, history = train_model(model, train_loader, test_loader, EPOCHS, timesteps=timesteps)\n",
        "    train_time = time.time() - t0\n",
        "    \n",
        "    results[name] = {\n",
        "        'accuracy': best_acc,\n",
        "        'spike_rate': history['spike_rate'][-1],\n",
        "        'parameters': params,\n",
        "        'timesteps': timesteps,\n",
        "        'time': train_time,\n",
        "        'history': history\n",
        "    }\n",
        "    \n",
        "    print(f'\\n{name}: {best_acc:.2f}% ({train_time:.1f}s)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '=' * 70)\n",
        "print('MNIST BENCHMARK RESULTS')\n",
        "print('=' * 70)\n",
        "print(f'{\"Model\":<15} {\"Accuracy\":<12} {\"Spike Rate\":<12} {\"Params\":<12} {\"Time\":<10}')\n",
        "print('-' * 70)\n",
        "\n",
        "for name, data in sorted(results.items(), key=lambda x: -x[1]['accuracy']):\n",
        "    print(f'{name:<15} {data[\"accuracy\"]:.2f}% {data[\"spike_rate\"]:.4f} {data[\"parameters\"]:>10,} {data[\"time\"]:.1f}s')\n",
        "\n",
        "print('=' * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for name, data in results.items():\n",
        "    axes[0].plot(data['history']['acc'], label=name)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Test Accuracy (%)')\n",
        "axes[0].set_title('Model Comparison: Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "names = list(results.keys())\n",
        "accs = [results[n]['accuracy'] for n in names]\n",
        "spikes = [results[n]['spike_rate'] for n in names]\n",
        "\n",
        "x = np.arange(len(names))\n",
        "width = 0.35\n",
        "\n",
        "ax1 = axes[1]\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "bars1 = ax1.bar(x - width/2, accs, width, label='Accuracy', color='steelblue')\n",
        "bars2 = ax2.bar(x + width/2, spikes, width, label='Spike Rate', color='coral')\n",
        "\n",
        "ax1.set_xlabel('Model')\n",
        "ax1.set_ylabel('Accuracy (%)', color='steelblue')\n",
        "ax2.set_ylabel('Spike Rate', color='coral')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(names, rotation=45, ha='right')\n",
        "ax1.set_title('Accuracy vs Spike Rate')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('all_models_comparison.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "results_save = {\n",
        "    name: {k: v for k, v in data.items() if k != 'history'}\n",
        "    for name, data in results.items()\n",
        "}\n",
        "\n",
        "with open('mnist_benchmark_results.json', 'w') as f:\n",
        "    json.dump(results_save, f, indent=2)\n",
        "\n",
        "print('Results saved to mnist_benchmark_results.json')"
      ]
    }
  ]
}
